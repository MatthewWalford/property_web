{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant Python libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from random_user_agent.user_agent import UserAgent\n",
    "from random_user_agent.params import SoftwareName, OperatingSystem\n",
    "\n",
    "import time\n",
    "from random import random\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to generate a randomised user agent for each connection \n",
    "def agent_randomiser():\n",
    "    software_names = [SoftwareName.CHROME.value]\n",
    "    operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value]   \n",
    "\n",
    "    user_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=100)\n",
    "\n",
    "    # Get list of user agents.\n",
    "    user_agents = user_agent_rotator.get_user_agents()\n",
    "\n",
    "    # Get Random User Agent String.\n",
    "    user_agent = user_agent_rotator.get_random_user_agent()\n",
    "    \n",
    "    user_agent2 = {\"User-Agent\": user_agent}\n",
    "    \n",
    "    return user_agent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify suburb; note if multiple suburbs are needed then currently need to run each independently \n",
    "#ensure that each suburb, state and postcode align correctly. \n",
    "suburbs = ['docklands']\n",
    "state = ['vic']\n",
    "postcode = ['3008']\n",
    "\n",
    "# create a list of root urls of suburbs to scrape\n",
    "if len(suburbs) == 1:   \n",
    "    root_url =  'https://www.domain.com.au/sold-listings/%s-%s-%s/?excludepricewithheld=1&ssubs=0' %(suburbs[0],state[0],postcode[0])\n",
    "else:\n",
    "    root_url = []\n",
    "    for i,suburb in enumerate(suburbs):\n",
    "        root_url.append('https://www.domain.com.au/sold-listings/%s-%s-%s/?excludepricewithheld=1&ssubs=0' %(suburb,state[i],postcode[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from website: 200\n",
      "Pages to Search: 74\n",
      "Appended to the Search dictionary\n",
      "\n",
      " {'docklands': {'Suburb': 'docklands', 'Total properties': '1479', 'Pages to Search': 74, 'Root Url': 'https://www.domain.com.au/sold-listings/docklands-vic-3008/?excludepricewithheld=1&ssubs=0', 'Website Response': 200}}\n"
     ]
    }
   ],
   "source": [
    "#Obtains information regarding what to extract. \n",
    "search={}\n",
    "\n",
    "res = requests.get(root_url, headers=agent_randomiser())\n",
    "\n",
    "if res.status_code == 200:\n",
    "    print(\"Response from website:\",res.status_code)\n",
    "\n",
    "    soup_the_page = BeautifulSoup(res.content,features='lxml')\n",
    "    properties_total_return = soup_the_page.find_all('h1',{'data-testid': 'summary'})\n",
    "    total_properties = properties_total_return[0].text.split()[0]\n",
    "\n",
    "    #This rounds up the number of URLs to srape from the base url\n",
    "    total_pages = -(-(int(total_properties))//20)\n",
    "    print(\"Pages to Search:\",total_pages)\n",
    "\n",
    "    #This is where the extracted information is appended to the search dictionary for use in later processing. \n",
    "    search.update({suburbs[0]: {'Suburb': suburbs[0],\n",
    "                                'Total properties':total_properties,\n",
    "                                'Pages to Search':total_pages,\n",
    "                                'Root Url':root_url,\n",
    "                                'Website Response': res.status_code}})\n",
    "\n",
    "    print(\"Appended to the Search dictionary\")\n",
    "else:\n",
    "    print(\"Response from website:\",res.status_code)\n",
    "    print(\"Something went wrong.\")\n",
    "    \n",
    "    search.update({suburbs[0]: {'Suburb': 'UNABLE TO EXTRACT',\n",
    "                                'Total properties':'UNABLE TO EXTRACT',\n",
    "                                'Pages to Search':'UNABLE TO EXTRACT',\n",
    "                                'Root Url':root_url,\n",
    "                                'Website Response': res.status_code}})\n",
    "\n",
    "print('\\n',search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates the list of specific properties to iterate through\n",
    "\n",
    "for k,value in search.items():\n",
    "#     print(k,value)\n",
    "    web_url_iterations = []\n",
    "    #This get the root URL and drops the final character which is a 0 for the page\n",
    "    url = value.get('Root Url') + str('&page=') #[:-1]\n",
    "    \n",
    "    if value.get('Pages to Search') > 50:\n",
    "        web_url_iterations.append(value.get('Root Url'))\n",
    "        #Note that the maximum amount of pages that Domain allow to view for sold is 50\n",
    "        #THIS HAS BEEN SET TO 50 to prevent errors. For testing, recommend set to five to speed up extraction and check information\n",
    "        iterations = 5                             \n",
    "        for i in range(1,iterations):\n",
    "            web_url_iterations.append(str(url + str((i+1))))\n",
    "            value.update({'URL Search Page List': web_url_iterations})\n",
    "            \n",
    "    elif value.get('Pages to Search') == 1:\n",
    "        web_url_iterations.append(value.get('Root Url'))\n",
    "    \n",
    "    elif value.get('Pages to Search') == 'UNABLE TO EXTRACT':\n",
    "        print('Was unable to extract a root url. Check website response')\n",
    "        value.update({'URL Search Page List': 'Was unable to extract a root url. Check website response'})\n",
    "    \n",
    "    else:\n",
    "        web_url_iterations.append(value.get('Root Url'))\n",
    "        iterations = value.get('Pages to Search')\n",
    "        for i in range(1,iterations):\n",
    "            web_url_iterations.append(str(url + str((i+1))))\n",
    "            value.update({'URL Search Page List': web_url_iterations})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Attempting to extract unique property URLS from: https://www.domain.com.au/sold-listings/docklands-vic-3008/?excludepricewithheld=1&ssubs=0&page=5\n",
      "Response from website: 200\n"
     ]
    }
   ],
   "source": [
    "for k,value in search.items():\n",
    "    search_url = value.get('URL Search Page List')\n",
    "    unique_property_url = []\n",
    "    \n",
    "    for url in search_url:\n",
    "        clear_output(wait=True)\n",
    "        print('------------')\n",
    "        print('Attempting to extract unique property URLS from:', url)\n",
    "        \n",
    "        time.sleep(random()*4)\n",
    "        \n",
    "        content = requests.get(url, headers=agent_randomiser())\n",
    "        print(\"Response from website:\",content.status_code)\n",
    "        \n",
    "        if content.status_code == 200:\n",
    "            property_pane = BeautifulSoup(content.content,features='lxml')\n",
    "            unique_links = property_pane.find_all('link',{'itemprop':'url'})\n",
    "            for a in unique_links:\n",
    "                unique_property_url.append(str(a).split('\"')[1])\n",
    "            value.update({'Specific property URLs': unique_property_url})\n",
    "        else:\n",
    "            print('An issue with the website occured. Please check website status code')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_property_brochure = []\n",
    "\n",
    "for k,value in search.items():\n",
    "    list_values = value.get('Specific property URLs')\n",
    "    \n",
    "    for pid in list_values:\n",
    "        unique_id = pid.split(\"-\")[-1]\n",
    "        brochure_url =  'https://www.domain.com.au/Public/PropertyBrochure.aspx?adid=%s&mode=sold' %(unique_id)\n",
    "        unique_property_brochure.append(brochure_url)\n",
    "#         print(brochure_url)\n",
    "        \n",
    "    value.update({'Brochure Links':unique_property_brochure})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 of 100\n",
      "Contacting https://www.domain.com.au/1015-673-la-trobe-street-docklands-vic-3008-2011168617\n",
      "Response from website: 200\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "all_properties=[]\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for k,value in search.items():\n",
    "    print('\\nGetting information for',str(value.get('Suburb')).upper())\n",
    "    print('---------------')\n",
    "    property_urls_suburb = value.get('Brochure Links')\n",
    "    \n",
    "    property_original_urls = value.get('Specific property URLs')\n",
    "#     print('List of property links for', str(value.get('Suburb')).upper())\n",
    "#     pprint(property_urls_suburb)\n",
    "    \n",
    "    for p,i in enumerate(property_urls_suburb):\n",
    "#         print(i)\n",
    "        time.sleep(random()*5)\n",
    "        clear_output(wait=True)\n",
    "        property_info = {}\n",
    "        \n",
    "        print('Connecting to ', i)\n",
    "        print(p+1,'of',len(property_urls_suburb))\n",
    "        web_response3 = requests.get(i, headers=agent_randomiser())\n",
    "        print(\"Response from website:\",web_response3.status_code)\n",
    "        if web_response3.status_code == 200:\n",
    "            print(\"Successfully extracted information\")\n",
    "            property_extract = BeautifulSoup(web_response3.content,features='lxml')                    \n",
    "            property_summary_pane = property_extract.find_all('div',{'style':'margin: 0 10px 10px'})\n",
    "            \n",
    "            #Search date for appending\n",
    "            search_date_time = datetime.now()\n",
    "            property_info['search_date'] = search_date_time.strftime(\"%d %b %y %T\")\n",
    "            \n",
    "            #Original url\n",
    "            property_info['url'] = i\n",
    "            \n",
    "            #Property address\n",
    "            address = property_extract.find('h2').text.split(\"\\r\")[0]\n",
    "            property_info['address'] = address\n",
    "            \n",
    "            property_info['state'] = state[0]\n",
    "            \n",
    "            #Property suburb\n",
    "            property_info['suburb'] = k\n",
    "            \n",
    "            #Property sale value\n",
    "            try:\n",
    "                sold_price = int(property_extract.find('h1').text.split(\"$\")[1].replace(\",\",\"\"))\n",
    "                property_info['sold_price'] = sold_price\n",
    "            except:\n",
    "                property_info['sold_price'] = 'unknown'\n",
    "            \n",
    "            #Extract beds, baths, carspaces, and Land Area\n",
    "            appending_info = []            \n",
    "            \n",
    "            strong_list = []\n",
    "\n",
    "            for strong_tag in property_summary_pane[0].find_all('strong'):\n",
    "            #     print(strong_tag.next_sibling.split())\n",
    "                strong_list.append(strong_tag.next_sibling.split())\n",
    "\n",
    "            for x in strong_list:\n",
    "                if len(x) == 1:\n",
    "                    pass\n",
    "                elif len(x)>1:\n",
    "                    property_info['bedrooms'] = 'unknown'\n",
    "                    property_info['bathrooms'] = 'unknown'\n",
    "                    property_info['carspaces'] = 'unknown'\n",
    "                    for i,o in enumerate(x):\n",
    "                        if o == 'bedrooms' or o == 'bedroom':\n",
    "                            property_info['bedrooms'] = int(x[i-1])\n",
    "                        if o == 'bathrooms' or o == 'bathroom':\n",
    "                            property_info['bathrooms'] = int(x[i-1])\n",
    "                        if o == 'carspace' or o == 'carspaces':\n",
    "                            property_info['carspaces'] = int(x[i-1])  \n",
    "    \n",
    "            \n",
    "            for st in property_extract.find_all('strong'):\n",
    "                property_info['land area'] = 'unknown'\n",
    "                if st.text == \"Land area:\":\n",
    "                    property_info['land area'] = float(st.next_sibling.split()[0])\n",
    "#                     print(float(st.next_sibling.split()[0]))\n",
    "\n",
    "            #This gets the description for the property from the brochure page\n",
    "            \n",
    "            try:\n",
    "                h3_list = []\n",
    "\n",
    "                for h3_tag in property_extract.find_all('h3',{'class':'agentheading'}):\n",
    "                    h3_list.append(h3_tag.next_sibling.next_sibling.next_sibling.next_sibling.next_sibling)\n",
    "\n",
    "                for h3 in h3_list:\n",
    "                    if len(str(h3).split()) > 30:\n",
    "                        property_description = str(h3_list[2]).replace(\"<p>\",\"\").replace(\"\\r\",\"\").replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"</p>\",\"\")\n",
    "                        property_info['description'] = property_description\n",
    "            except:\n",
    "                property_info['description'] = 'No description extracted'\n",
    "            \n",
    "            all_properties.append(property_info)\n",
    "            print('****')\n",
    "        else:\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!\") #CHANGE THIS TO INSERT UNKNOWN INFORMATION FOR A URL\n",
    "            print(\"Something went wrong!\")\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "#Sale date need to come from the original page. This collects this information            \n",
    "print('-------------------------------------')\n",
    "print('NOW COLLLECTING SALE DATE INFORMATION')\n",
    "print('-------------------------------------')\n",
    "\n",
    "for pid,property_id in enumerate(all_properties):\n",
    "        \n",
    "    time.sleep(random()*5)\n",
    "    property_info_2 = {}\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(pid+1,'of',len(all_properties))\n",
    "    \n",
    "    web_response4 = requests.get(search[suburbs[0]]['Specific property URLs'][pid], headers=agent_randomiser())\n",
    "    print(\"Contacting\", search[suburbs[0]]['Specific property URLs'][pid])\n",
    "    print(\"Response from website:\",web_response4.status_code)\n",
    "    \n",
    "    if web_response4.status_code == 200:\n",
    "        property_extract_2 = BeautifulSoup(web_response4.content,features='lxml')                    \n",
    "        property_summary_pane_2 = property_extract_2.find_all('span',{'class':'listing-details__listing-tag is-sold listing-details__summary-tag'})\n",
    "        \n",
    "        day_sold = property_summary_pane_2[0].text.split()[-3:][0]\n",
    "        month_sold = property_summary_pane_2[0].text.split()[-3:][1]\n",
    "        year_sold = int(property_summary_pane_2[0].text.split()[-3:][2])\n",
    "        \n",
    "        all_properties[pid]['day sold'] = day_sold\n",
    "        all_properties[pid]['month sold'] = month_sold\n",
    "        all_properties[pid]['year sold'] = year_sold\n",
    "        \n",
    "        property_type = property_extract_2.find('span',{'class':'listing-details__property-type-features-text'}).text\n",
    "        all_properties[pid]['type'] = property_type\n",
    "        \n",
    "        try:\n",
    "            if property_id['sold_price'] == 'unknown':\n",
    "                property_id['sold_price'] = int(property_extract_2.find('div',{'class':'listing-details__summary-title','data-testid':'listing-details__summary-title'}).text.split(\"$\")[1].replace(\",\",\"\"))\n",
    "        except:\n",
    "            property_id['sold_price'] = 'unknown'\n",
    "    \n",
    "    else:\n",
    "        all_properties[pid]['day sold'] = 'unknown'\n",
    "        all_properties[pid]['month sold'] = 'unknown'\n",
    "        all_properties[pid]['year sold'] = 'unknown'\n",
    "        \n",
    "        \n",
    "    print('****')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Save search as a file as excel for use later if required. \n",
    "path = \"./previous_searches\"\n",
    "if os.path.exists(path):\n",
    "    pass \n",
    "else:\n",
    "    os.mkdir(path)\n",
    "\n",
    "df_previous = pd.DataFrame(all_properties)\n",
    "\n",
    "date = datetime.now()\n",
    "filename_sold = path + \"/\" + date.strftime(\"%d%b%y_%T\").replace(\":\",\"\")  + \"_\" + str(suburbs[0]) + \"_\" + str(state[0]) + \"_\" + str(postcode[0]) + \"_sold.xlsx\"\n",
    "sheet_name = str(suburbs[0]) + str(postcode[0]) + \" SOLD\"\n",
    "\n",
    "df_previous.to_excel(filename_sold,sheet_name=sheet_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
